{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as web\n",
    "#import tensorflow as tf\n",
    "import chainer.functions  as F\n",
    "from chainer import FunctionSet\n",
    "from chainer import Variable\n",
    "from chainer import optimizers\n",
    "#from chainer import cuda\n",
    "from datetime import datetime\n",
    "#from sklearn.datasets import fetch_mldata\n",
    "import model_util\n",
    "models=model_util.ModelUtil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_data(start, end):\n",
    "    # ヒストリカルデータをダウンロードする。\n",
    "    snp = web.DataReader('^GSPC', 'yahoo', start, end)\n",
    "    nyse = web.DataReader('^NYA', 'yahoo', start, end)\n",
    "    djia = web.DataReader('^DJI', 'yahoo', start, end)\n",
    "    nikkei = web.DataReader('^N225', 'yahoo', start, end)\n",
    "    hangseng = web.DataReader('000001.SS', 'yahoo', start, end)\n",
    "    ftse = web.DataReader('ISF.L', 'yahoo', start, end)  # ETFで代用。\n",
    "    dax = web.DataReader('^GDAXI', 'yahoo', start, end)\n",
    "    aord = web.DataReader('^AORD', 'yahoo', start, end)\n",
    "\n",
    "    # 終値を格納する。\n",
    "    closing_data = pd.DataFrame()\n",
    "    closing_data['snp_close'] = snp['Close']\n",
    "    closing_data['nyse_close'] = nyse['Close']\n",
    "    closing_data['djia_close'] = djia['Close']\n",
    "    closing_data['nikkei_close'] = nikkei['Close']\n",
    "    closing_data['hangseng_close'] = hangseng['Close']\n",
    "    closing_data['ftse_close'] = ftse['Close']\n",
    "    closing_data['dax_close'] = dax['Close']\n",
    "    closing_data['aord_close'] = aord['Close']\n",
    "\n",
    "    # 終値の欠損値を前のデータで補間する。\n",
    "    closing_data = closing_data.fillna(method='ffill')\n",
    "\n",
    "    # 終値の対数変化率を格納する。\n",
    "    log_return_data = pd.DataFrame()\n",
    "    log_return_data['snp_log_return'] = (np.log(closing_data['snp_close'] /\n",
    "        closing_data['snp_close'].shift()))\n",
    "    log_return_data['nyse_log_return'] = (np.log(closing_data['nyse_close'] /\n",
    "        closing_data['nyse_close'].shift()))\n",
    "    log_return_data['djia_log_return'] = (np.log(closing_data['djia_close'] /\n",
    "        closing_data['djia_close'].shift()))\n",
    "    log_return_data['nikkei_log_return'] = (np.log(closing_data['nikkei_close']\n",
    "        / closing_data['nikkei_close'].shift()))\n",
    "    log_return_data['hangseng_log_return'] = (\n",
    "        np.log(closing_data['hangseng_close'] /\n",
    "        closing_data['hangseng_close'].shift()))\n",
    "    log_return_data['ftse_log_return'] = (np.log(closing_data['ftse_close'] /\n",
    "        closing_data['ftse_close'].shift()))\n",
    "    log_return_data['dax_log_return'] = (np.log(closing_data['dax_close'] /\n",
    "        closing_data['dax_close'].shift()))\n",
    "    log_return_data['aord_log_return'] = (np.log(closing_data['aord_close'] /\n",
    "        closing_data['aord_close'].shift()))\n",
    "\n",
    "    # S&P500の対数変化率が0以上なら1、さもなければ0を格納した列を加える。\n",
    "    log_return_data['snp_log_return_positive'] = 0\n",
    "    log_return_data.ix[log_return_data['snp_log_return'] >= 0,\n",
    "                       'snp_log_return_positive'] = 1\n",
    "\n",
    "    # S&P500の対数変化率が0未満なら1、さもなければ0を格納した列を加える。\n",
    "    log_return_data['snp_log_return_negative'] = 0\n",
    "    log_return_data.ix[log_return_data['snp_log_return'] < 0,\n",
    "                       'snp_log_return_negative'] = 1\n",
    "\n",
    "    # 学習・テスト用データを作成する。\n",
    "    training_test_data = pd.DataFrame(\n",
    "        columns=[\n",
    "            'snp_log_return_positive', 'snp_log_return_negative',\n",
    "            'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "            'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "            'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "            'nikkei_log_return_1', 'nikkei_log_return_2', 'nikkei_log_return_3',\n",
    "            'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "                'hangseng_log_return_3',\n",
    "            'ftse_log_return_1', 'ftse_log_return_2', 'ftse_log_return_3',\n",
    "            'dax_log_return_1', 'dax_log_return_2', 'dax_log_return_3',\n",
    "            'aord_log_return_1', 'aord_log_return_2', 'aord_log_return_3'])\n",
    "    for i in range(7, len(log_return_data)):\n",
    "        snp_log_return_positive = (\n",
    "            log_return_data['snp_log_return_positive'].ix[i])\n",
    "        snp_log_return_negative = (\n",
    "            log_return_data['snp_log_return_negative'].ix[i])\n",
    "\n",
    "        # 先読みバイアスを排除するため、当日のデータを使わない。\n",
    "        snp_log_return_1 = log_return_data['snp_log_return'].ix[i]\n",
    "        snp_log_return_2 = log_return_data['snp_log_return'].ix[i-1]\n",
    "        snp_log_return_3 = log_return_data['snp_log_return'].ix[i-2]\n",
    "        nyse_log_return_1 = log_return_data['nyse_log_return'].ix[i]\n",
    "        nyse_log_return_2 = log_return_data['nyse_log_return'].ix[i-1]\n",
    "        nyse_log_return_3 = log_return_data['nyse_log_return'].ix[i-2]\n",
    "        djia_log_return_1 = log_return_data['djia_log_return'].ix[i]\n",
    "        djia_log_return_2 = log_return_data['djia_log_return'].ix[i-1]\n",
    "        djia_log_return_3 = log_return_data['djia_log_return'].ix[i-2]\n",
    "        nikkei_log_return_1 = log_return_data['nikkei_log_return'].ix[i]\n",
    "        nikkei_log_return_2 = log_return_data['nikkei_log_return'].ix[i-1]\n",
    "        nikkei_log_return_3 = log_return_data['nikkei_log_return'].ix[i-2]\n",
    "        hangseng_log_return_1 = log_return_data['hangseng_log_return'].ix[i]\n",
    "        hangseng_log_return_2 = log_return_data['hangseng_log_return'].ix[i-1]\n",
    "        hangseng_log_return_3 = log_return_data['hangseng_log_return'].ix[i-2]\n",
    "        ftse_log_return_1 = log_return_data['ftse_log_return'].ix[i]\n",
    "        ftse_log_return_2 = log_return_data['ftse_log_return'].ix[i-1]\n",
    "        ftse_log_return_3 = log_return_data['ftse_log_return'].ix[i-2]\n",
    "        dax_log_return_1 = log_return_data['dax_log_return'].ix[i]\n",
    "        dax_log_return_2 = log_return_data['dax_log_return'].ix[i-1]\n",
    "        dax_log_return_3 = log_return_data['dax_log_return'].ix[i-2]\n",
    "        aord_log_return_1 = log_return_data['aord_log_return'].ix[i]\n",
    "        aord_log_return_2 = log_return_data['aord_log_return'].ix[i-1]\n",
    "        aord_log_return_3 = log_return_data['aord_log_return'].ix[i-2]\n",
    "\n",
    "        # 各データをインデックスのラベルを使用しないで結合する。\n",
    "        training_test_data = training_test_data.append(\n",
    "            {'snp_log_return_positive':snp_log_return_positive,\n",
    "            'snp_log_return_negative':snp_log_return_negative,\n",
    "            'snp_log_return_1':snp_log_return_1,\n",
    "            'snp_log_return_2':snp_log_return_2,\n",
    "            'snp_log_return_3':snp_log_return_3,\n",
    "            'nyse_log_return_1':nyse_log_return_1,\n",
    "            'nyse_log_return_2':nyse_log_return_2,\n",
    "            'nyse_log_return_3':nyse_log_return_3,\n",
    "            'djia_log_return_1':djia_log_return_1,\n",
    "            'djia_log_return_2':djia_log_return_2,\n",
    "            'djia_log_return_3':djia_log_return_3,\n",
    "            'nikkei_log_return_1':nikkei_log_return_1,\n",
    "            'nikkei_log_return_2':nikkei_log_return_2,\n",
    "            'nikkei_log_return_3':nikkei_log_return_3,\n",
    "            'hangseng_log_return_1':hangseng_log_return_1,\n",
    "            'hangseng_log_return_2':hangseng_log_return_2,\n",
    "            'hangseng_log_return_3':hangseng_log_return_3,\n",
    "            'ftse_log_return_1':ftse_log_return_1,\n",
    "            'ftse_log_return_2':ftse_log_return_2,\n",
    "            'ftse_log_return_3':ftse_log_return_3,\n",
    "            'dax_log_return_1':dax_log_return_1,\n",
    "            'dax_log_return_2':dax_log_return_2,\n",
    "            'dax_log_return_3':dax_log_return_3,\n",
    "            'aord_log_return_1':aord_log_return_1,\n",
    "            'aord_log_return_2':aord_log_return_2,\n",
    "            'aord_log_return_3':aord_log_return_3},\n",
    "            ignore_index=True)\n",
    "\n",
    "    # 3列目以降を説明変数として格納する。\n",
    "    predictors_tf = training_test_data[training_test_data.columns[2:]]\n",
    "\n",
    "    # 1、2列目を目的変数として格納する。\n",
    "    classes_tf = training_test_data[training_test_data.columns[:2]]\n",
    "\n",
    "    # 学習用セットのサイズを学習・テスト用データの80%に設定する。\n",
    "    training_set_size = int(len(training_test_data) * 0.8)\n",
    "\n",
    "    # 説明変数の初めの80%を学習用データにする。\n",
    "    x_train = predictors_tf[:training_set_size].as_matrix()\n",
    "    \n",
    "    # 目的変数の初めの80%を学習用データにする。\n",
    "    y_train = classes_tf[:training_set_size].as_matrix()\n",
    "\n",
    "    # 説明変数の残りの20%をテスト用データにする。\n",
    "    x_test = predictors_tf[training_set_size:].as_matrix()\n",
    "\n",
    "    # 目的変数の残りの20%をテスト用データにする。\n",
    "    y_test = classes_tf[training_set_size:].as_matrix()\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, training_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ニューラルネットの構造\n",
    "def forward(x, train=True):\n",
    "    # 活性化関数を選択\n",
    "    # sigmoid関数\n",
    "#     h1 = F.sigmoid(model.l1(x))\n",
    "#     h2 = F.sigmoid(model.l2(h1))\n",
    "\n",
    "    # dropoutを追加\n",
    "    # ReLU関数\n",
    "    h1 = F.dropout(F.relu(model.l1(x)),  train=train)\n",
    "    h2 = F.dropout(F.relu(model.l2(h1)), train=train)\n",
    "    y  = model.l3(h2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x_batch, y_batch, train=True):\n",
    "    # 型を変換\n",
    "    x = Variable(x_batch.astype(np.float32), volatile=False) \n",
    "    t = Variable(y_batch.astype(np.float32), volatile=False)\n",
    "    p = forward(x, train)\n",
    "\n",
    "    # 勾配を初期化\n",
    "    optimizer.zero_grads()\n",
    "#     print(\"x: ndim {0} data {1}\".format(x.ndim,x.data))\n",
    "#     print(\"p: ndim {0} data {1}\".format(p.ndim,p.data))\n",
    "#     print(\"p_class: {}\".format(p.data.argmax()))\n",
    "\n",
    "    # 順伝播させて誤差と精度を算出\n",
    "    # 多クラス分類なので誤差関数としてソフトマックス関数の\n",
    "    # 交差エントロピー関数を用いて、誤差を導出\n",
    "#     loss = F.softmax_cross_entropy(p, t)\n",
    "    loss = F.mean_squared_error(p, t)\n",
    "#     acc = F.accuracy(p, t)\n",
    "\n",
    "    # 誤差逆伝播で勾配を計算\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "#     return loss, acc\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ヒストリカルデータの開始日と終了日を設定する。\n",
    "start = datetime(2010, 1, 1)\n",
    "end = datetime(2016, 11, 23)\n",
    "\n",
    "x_train, x_test, y_train, y_test, N = set_data(start,end)\n",
    "\n",
    "N_test = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ターゲットモデル名\n",
    "target='kabu_price'\n",
    "\n",
    "# Prepare multi-layer perceptron model\n",
    "# 多層パーセプトロンモデルの設定\n",
    "# 入力 784次元、出力 10次元\n",
    "# 中間層のunit数\n",
    "# 一層目\n",
    "n_units = 120\n",
    "# 二層目\n",
    "n2_units = 240\n",
    "\n",
    "model = FunctionSet(l1=F.Linear(24, n_units),\n",
    "                    l2=F.Linear(n_units, n2_units),\n",
    "                    l3=F.Linear(n2_units, 2))\n",
    "model.compute_accuracy = False #accuracyを計算しない\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "train_loss = []\n",
    "train_acc  = []\n",
    "test_loss = []\n",
    "test_acc  = []\n",
    "\n",
    "l1_W = []\n",
    "l2_W = []\n",
    "l3_W = []\n",
    "\n",
    "# 確率的勾配降下法で学習させる際の１回分のバッチサイズ\n",
    "batchsize = 4\n",
    "\n",
    "# 学習の繰り返し回数\n",
    "n_epoch = 20\n",
    "#n_epoch   = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "save the model\n",
      "save the optimizer\n"
     ]
    }
   ],
   "source": [
    "# Learning loop\n",
    "for epoch in xrange(1, n_epoch+1):\n",
    "    print 'epoch', epoch\n",
    "\n",
    "    # training\n",
    "    # N個の順番をランダムに並び替える\n",
    "    perm = np.random.permutation(N)\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "\n",
    "    # 0〜Nまでのデータをバッチサイズごとに使って学習\n",
    "    for i in xrange(0, N, batchsize):\n",
    "        x_batch = x_train[perm[i:i+batchsize]]\n",
    "        y_batch = y_train[perm[i:i+batchsize]]\n",
    "#         loss, acc = train(x_batch, y_batch)\n",
    "        loss = train(x_batch, y_batch)\n",
    "\n",
    "#         sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n",
    "#         sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n",
    "        sum_loss     += float(loss.data) * batchsize\n",
    "#         sum_accuracy += float(acc.data) * batchsize\n",
    "\n",
    "    # 訓練データの誤差と、正解精度を表示\n",
    "    #print 'train mean loss={}, accuracy={}'.format(sum_loss / N, sum_accuracy / N)\n",
    "\n",
    "    train_loss.append(sum_loss / N)\n",
    "#     train_acc.append(sum_accuracy / N)\n",
    "\n",
    "    # evaluation\n",
    "    # テストデータで誤差と、正解精度を算出し汎化性能を確認\n",
    "#     sum_accuracy = 0\n",
    "    sum_loss     = 0\n",
    "    for i in xrange(0, N_test, batchsize):\n",
    "        x_batch = x_test[i:i+batchsize]\n",
    "        y_batch = y_test[i:i+batchsize]\n",
    "#         loss, acc = train(x_batch, y_batch)\n",
    "        loss = train(x_batch, y_batch)\n",
    "\n",
    "#         sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n",
    "#         sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n",
    "        sum_loss     += float(loss.data) * batchsize\n",
    "#         sum_accuracy += float(acc.data) * batchsize\n",
    "\n",
    "    # テストデータでの誤差と、正解精度を表示\n",
    "    # print 'test  mean loss={}'.format(sum_loss / N_test)\n",
    "#     print 'test  mean loss={}, accuracy={}'.format(sum_loss / N_test, sum_accuracy / N_test)\n",
    "    test_loss.append(sum_loss / N_test)\n",
    "#     test_acc.append(sum_accuracy / N_test)\n",
    "\n",
    "    # 学習したパラメーターを保存\n",
    "    l1_W.append(model.l1.W)\n",
    "    l2_W.append(model.l2.W)\n",
    "    l3_W.append(model.l3.W)\n",
    "    \n",
    "models.set_model_pkl('{0}.pkl'.format(target))\n",
    "models.set_model_name('{0}.model'.format(target))\n",
    "models.set_optimizer_name('{0}.state'.format(target))\n",
    "models.dump_model(model)\n",
    "models.dump_model_and_optimizer(model, optimizer)\n",
    "    \n",
    "# 精度と誤差をグラフ描画\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.plot(range(len(train_acc)), train_acc)\n",
    "# plt.plot(range(len(test_acc)), test_acc)\n",
    "# plt.legend([\"train_acc\",\"test_acc\"],loc=4)\n",
    "# plt.title(\"Accuracy of digit recognition.\")\n",
    "# plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_production_data(start, end):\n",
    "    # ヒストリカルデータをダウンロードする。\n",
    "    snp = web.DataReader('^GSPC', 'yahoo', start, end)\n",
    "    nyse = web.DataReader('^NYA', 'yahoo', start, end)\n",
    "    djia = web.DataReader('^DJI', 'yahoo', start, end)\n",
    "    nikkei = web.DataReader('^N225', 'yahoo', start, end)\n",
    "    hangseng = web.DataReader('000001.SS', 'yahoo', start, end)\n",
    "    ftse = web.DataReader('ISF.L', 'yahoo', start, end)  # ETFで代用。\n",
    "    dax = web.DataReader('^GDAXI', 'yahoo', start, end)\n",
    "    aord = web.DataReader('^AORD', 'yahoo', start, end)\n",
    "\n",
    "    # label格納\n",
    "    label_data = nikkei.index\n",
    "    \n",
    "    # 終値を格納する。\n",
    "    closing_data = pd.DataFrame()\n",
    "    closing_data['snp_close'] = snp['Close']\n",
    "    closing_data['nyse_close'] = nyse['Close']\n",
    "    closing_data['djia_close'] = djia['Close']\n",
    "    closing_data['nikkei_close'] = nikkei['Close']\n",
    "    closing_data['hangseng_close'] = hangseng['Close']\n",
    "    closing_data['ftse_close'] = ftse['Close']\n",
    "    closing_data['dax_close'] = dax['Close']\n",
    "    closing_data['aord_close'] = aord['Close']\n",
    "\n",
    "    # 終値の欠損値を前のデータで補間する。\n",
    "    closing_data = closing_data.fillna(method='ffill')\n",
    "\n",
    "    # 終値の対数変化率を格納する。\n",
    "    log_return_data = pd.DataFrame()\n",
    "    log_return_data['snp_log_return'] = (np.log(closing_data['snp_close'] /\n",
    "        closing_data['snp_close'].shift()))\n",
    "    log_return_data['nyse_log_return'] = (np.log(closing_data['nyse_close'] /\n",
    "        closing_data['nyse_close'].shift()))\n",
    "    log_return_data['djia_log_return'] = (np.log(closing_data['djia_close'] /\n",
    "        closing_data['djia_close'].shift()))\n",
    "    log_return_data['nikkei_log_return'] = (np.log(closing_data['nikkei_close']\n",
    "        / closing_data['nikkei_close'].shift()))\n",
    "    log_return_data['hangseng_log_return'] = (\n",
    "        np.log(closing_data['hangseng_close'] /\n",
    "        closing_data['hangseng_close'].shift()))\n",
    "    log_return_data['ftse_log_return'] = (np.log(closing_data['ftse_close'] /\n",
    "        closing_data['ftse_close'].shift()))\n",
    "    log_return_data['dax_log_return'] = (np.log(closing_data['dax_close'] /\n",
    "        closing_data['dax_close'].shift()))\n",
    "    log_return_data['aord_log_return'] = (np.log(closing_data['aord_close'] /\n",
    "        closing_data['aord_close'].shift()))\n",
    "\n",
    "    # S&P500の対数変化率が0以上なら1、さもなければ0を格納した列を加える。\n",
    "    log_return_data['snp_log_return_positive'] = 0\n",
    "    log_return_data.ix[log_return_data['snp_log_return'] >= 0,\n",
    "                       'snp_log_return_positive'] = 1\n",
    "\n",
    "    # S&P500の対数変化率が0未満なら1、さもなければ0を格納した列を加える。\n",
    "    log_return_data['snp_log_return_negative'] = 0\n",
    "    log_return_data.ix[log_return_data['snp_log_return'] < 0,\n",
    "                       'snp_log_return_negative'] = 1\n",
    "\n",
    "    # 学習・テスト用データを作成する。\n",
    "    training_test_data = pd.DataFrame(\n",
    "        columns=[\n",
    "            'snp_log_return_positive', 'snp_log_return_negative',\n",
    "            'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "            'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "            'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "            'nikkei_log_return_1', 'nikkei_log_return_2', 'nikkei_log_return_3',\n",
    "            'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "                'hangseng_log_return_3',\n",
    "            'ftse_log_return_1', 'ftse_log_return_2', 'ftse_log_return_3',\n",
    "            'dax_log_return_1', 'dax_log_return_2', 'dax_log_return_3',\n",
    "            'aord_log_return_1', 'aord_log_return_2', 'aord_log_return_3'])\n",
    "\n",
    "    for i in range(7, len(log_return_data)):\n",
    "        snp_log_return_positive = (\n",
    "            log_return_data['snp_log_return_positive'].ix[i])\n",
    "        snp_log_return_negative = (\n",
    "            log_return_data['snp_log_return_negative'].ix[i])\n",
    "\n",
    "        # 先読みバイアスを排除するため、当日のデータを使わない。\n",
    "        snp_log_return_1 = log_return_data['snp_log_return'].ix[i]\n",
    "        snp_log_return_2 = log_return_data['snp_log_return'].ix[i-1]\n",
    "        snp_log_return_3 = log_return_data['snp_log_return'].ix[i-2]\n",
    "        nyse_log_return_1 = log_return_data['nyse_log_return'].ix[i]\n",
    "        nyse_log_return_2 = log_return_data['nyse_log_return'].ix[i-1]\n",
    "        nyse_log_return_3 = log_return_data['nyse_log_return'].ix[i-2]\n",
    "        djia_log_return_1 = log_return_data['djia_log_return'].ix[i]\n",
    "        djia_log_return_2 = log_return_data['djia_log_return'].ix[i-1]\n",
    "        djia_log_return_3 = log_return_data['djia_log_return'].ix[i-2]\n",
    "        nikkei_log_return_1 = log_return_data['nikkei_log_return'].ix[i]\n",
    "        nikkei_log_return_2 = log_return_data['nikkei_log_return'].ix[i-1]\n",
    "        nikkei_log_return_3 = log_return_data['nikkei_log_return'].ix[i-2]\n",
    "        hangseng_log_return_1 = log_return_data['hangseng_log_return'].ix[i]\n",
    "        hangseng_log_return_2 = log_return_data['hangseng_log_return'].ix[i-1]\n",
    "        hangseng_log_return_3 = log_return_data['hangseng_log_return'].ix[i-2]\n",
    "        ftse_log_return_1 = log_return_data['ftse_log_return'].ix[i]\n",
    "        ftse_log_return_2 = log_return_data['ftse_log_return'].ix[i-1]\n",
    "        ftse_log_return_3 = log_return_data['ftse_log_return'].ix[i-2]\n",
    "        dax_log_return_1 = log_return_data['dax_log_return'].ix[i]\n",
    "        dax_log_return_2 = log_return_data['dax_log_return'].ix[i-1]\n",
    "        dax_log_return_3 = log_return_data['dax_log_return'].ix[i-2]\n",
    "        aord_log_return_1 = log_return_data['aord_log_return'].ix[i]\n",
    "        aord_log_return_2 = log_return_data['aord_log_return'].ix[i-1]\n",
    "        aord_log_return_3 = log_return_data['aord_log_return'].ix[i-2]\n",
    "\n",
    "        # 各データをインデックスのラベルを使用しないで結合する。\n",
    "        training_test_data = training_test_data.append(\n",
    "            {\n",
    "            'snp_log_return_positive':snp_log_return_positive,\n",
    "            'snp_log_return_negative':snp_log_return_negative,\n",
    "            'snp_log_return_1':snp_log_return_1,\n",
    "            'snp_log_return_2':snp_log_return_2,\n",
    "            'snp_log_return_3':snp_log_return_3,\n",
    "            'nyse_log_return_1':nyse_log_return_1,\n",
    "            'nyse_log_return_2':nyse_log_return_2,\n",
    "            'nyse_log_return_3':nyse_log_return_3,\n",
    "            'djia_log_return_1':djia_log_return_1,\n",
    "            'djia_log_return_2':djia_log_return_2,\n",
    "            'djia_log_return_3':djia_log_return_3,\n",
    "            'nikkei_log_return_1':nikkei_log_return_1,\n",
    "            'nikkei_log_return_2':nikkei_log_return_2,\n",
    "            'nikkei_log_return_3':nikkei_log_return_3,\n",
    "            'hangseng_log_return_1':hangseng_log_return_1,\n",
    "            'hangseng_log_return_2':hangseng_log_return_2,\n",
    "            'hangseng_log_return_3':hangseng_log_return_3,\n",
    "            'ftse_log_return_1':ftse_log_return_1,\n",
    "            'ftse_log_return_2':ftse_log_return_2,\n",
    "            'ftse_log_return_3':ftse_log_return_3,\n",
    "            'dax_log_return_1':dax_log_return_1,\n",
    "            'dax_log_return_2':dax_log_return_2,\n",
    "            'dax_log_return_3':dax_log_return_3,\n",
    "            'aord_log_return_1':aord_log_return_1,\n",
    "            'aord_log_return_2':aord_log_return_2,\n",
    "            'aord_log_return_3':aord_log_return_3},\n",
    "            ignore_index=True)\n",
    "    \n",
    "    # 3列目以降を説明変数として格納する。\n",
    "    predictors_tf = training_test_data[training_test_data.columns[2:]]\n",
    "\n",
    "    # 1、2列目を目的変数として格納する。\n",
    "    classes_tf = training_test_data[training_test_data.columns[:2]] \n",
    "\n",
    "    # 説明変数の初めの80%を学習用データにする。\n",
    "    x_train = predictors_tf.as_matrix()\n",
    "    \n",
    "    # 目的変数の初めの80%を学習用データにする。\n",
    "    y_train = classes_tf.as_matrix()\n",
    "    \n",
    "    return x_train, y_train, nikkei.index.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modelの読み込み\n",
    "# models.set_model_pkl('{0}.pkl'.format(target))\n",
    "# models.set_model_name('{0}.model'.format(target))\n",
    "# models.set_optimizer_name('{0}.state'.format(target))\n",
    "# model=models.load_model()\n",
    "\n",
    "# ヒストリカルデータの開始日と終了日を設定する。\n",
    "start = datetime(2016, 11, 10)\n",
    "end = datetime(2016, 11, 24)\n",
    "nikkei = web.DataReader('^N225', 'yahoo', start, end)\n",
    "\n",
    "x, y, index = set_production_data(start,end)\n",
    "# print len(x)\n",
    "h1 = F.dropout(F.relu(model.l1(Variable(x.astype(np.float32)))),  train=False)\n",
    "h2 = F.dropout(F.relu(model.l2(h1)), train=False)\n",
    "p  = model.l3(h2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.48943156  0.50108069] [ 1.  0.] 2016-11-10 00:00:00\n",
      "[ 0.49722451  0.49589059] [ 1.  0.] 2016-11-11 00:00:00\n",
      "[ 0.48279443  0.50594825] [ 1.  0.] 2016-11-14 00:00:00\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# print np.argmax(p.data)\n",
    "for i, ans in enumerate(p.data):\n",
    "    print ans, y[i], index[i]\n",
    "print np.argmax(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
