{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テーマ\n",
    "  投資対象を絞り込む用途に用いる。\n",
    "  そのため、具体的な投資対象を一意に絞り込む用途は想定していない。\n",
    "\n",
    "### 用途目的\n",
    "  投資対象として有望なものを導出する。\n",
    "　１．一年分のチャート等を読み込ませ今後の動向を分類させる。（急騰、暴落、停滞、微増、微減）など"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from chainer import cuda\n",
    "from chainer import FunctionSet\n",
    "from chainer import Variable\n",
    "from chainer import optimizers\n",
    "import chainer.functions  as F\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "import model_util\n",
    "models=model_util.ModelUtil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_data(target):\n",
    "    if 'mnist' == target:\n",
    "        # MNISTの手書き数字データのダウンロード\n",
    "        # #HOME/scikit_learn_data/mldata/mnist-original.mat にキャッシュされる\n",
    "        mnist = fetch_mldata('MNIST original')\n",
    "        # mnist.data : 70,000件の784次元ベクトルデータ\n",
    "        # だいたい一つの数字につき7000件前後　手書き数字データ\n",
    "        mnist.data   = mnist.data.astype(np.float32)\n",
    "        mnist.data  /= 255     # 0-1のデータに変換\n",
    "\n",
    "        # mnist.target : 正解データ（教師データ）\n",
    "        mnist.target = mnist.target.astype(np.int32)\n",
    "\n",
    "        # 学習用データを N個、検証用データを残りの個数と設定\n",
    "        N = 56000\n",
    "    #     print(len(mnist.data))\n",
    "        x_train, x_test = np.split(mnist.data,   [N])\n",
    "        y_train, y_test = np.split(mnist.target, [N])\n",
    "    else:\n",
    "        print 'hogehoge'\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ニューラルネットの構造\n",
    "def forward(x_data, train=True):\n",
    "    # 活性化関数を選択\n",
    "    # sigmoid関数\n",
    "#     h1 = F.sigmoid(model.l1(x))\n",
    "#     h2 = F.sigmoid(model.l2(h1))\n",
    "\n",
    "    # dropoutを追加\n",
    "    # ReLU関数\n",
    "    h1 = F.dropout(F.relu(model.l1(x)),  train=train)\n",
    "    h2 = F.dropout(F.relu(model.l2(h1)), train=train)\n",
    "    y  = model.l3(h2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x_batch, y_batch, train=True):\n",
    "    # 型を変換\n",
    "    x = Variable(x_batch.astype(np.float32), volatile=False) \n",
    "    t = Variable(y_batch.astype(np.int32), volatile=False)\n",
    "    p = forward(x, train)\n",
    "\n",
    "    # 勾配を初期化\n",
    "    optimizer.zero_grads()\n",
    "    print(\"x: ndim {0} data {1}\".format(x.ndim,len(x.data)))\n",
    "    print(\"p: ndim {0} data {1}\".format(p.ndim,len(p.data)))\n",
    "    print(\"p_class: {}\".format(p.data.argmax()))\n",
    "\n",
    "    # 順伝播させて誤差と精度を算出\n",
    "    # 多クラス分類なので誤差関数としてソフトマックス関数の\n",
    "    # 交差エントロピー関数を用いて、誤差を導出\n",
    "    loss = F.softmax_cross_entropy(p, t)\n",
    "#         loss = F.mean_squared_error(p, t)\n",
    "    acc = F.accuracy(p, t)\n",
    "\n",
    "    # 誤差逆伝播で勾配を計算\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データ設定\n",
    "target='mnist'\n",
    "x_train, x_test, y_train, y_test, N = set_data(target)\n",
    "N_test = y_test.size\n",
    "    \n",
    "# Prepare multi-layer perceptron model\n",
    "# 多層パーセプトロンモデルの設定\n",
    "# 入力 784次元、出力 10次元\n",
    "# 中間層のunit数\n",
    "# 一層目\n",
    "n_units = 200\n",
    "# 二層目\n",
    "n2_units = 100\n",
    "\n",
    "model = FunctionSet(l1=F.Linear(784, n_units),\n",
    "                    l2=F.Linear(n_units, n2_units),\n",
    "                    l3=F.Linear(n2_units, 10))\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "train_loss = []\n",
    "train_acc  = []\n",
    "test_loss = []\n",
    "test_acc  = []\n",
    "\n",
    "l1_W = []\n",
    "l2_W = []\n",
    "l3_W = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "print len(y_test)\n",
    "print N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "x: ndim 2 data 100\n",
      "p: ndim 2 data 100\n",
      "p_class: 199\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0ab136524c7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0msum_loss\u001b[0m     \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0msum_accuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m# 訓練データの誤差と、正解精度を表示\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#print 'train mean loss={}, accuracy={}'.format(sum_loss / N, sum_accuracy / N)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 確率的勾配降下法で学習させる際の１回分のバッチサイズ\n",
    "batchsize = 100\n",
    "\n",
    "# 学習の繰り返し回数\n",
    "# n_epoch   = 20\n",
    "n_epoch   = 1\n",
    "# Learning loop\n",
    "for epoch in xrange(1, n_epoch+1):\n",
    "    print 'epoch', epoch\n",
    "\n",
    "    # training\n",
    "    # N個の順番をランダムに並び替える\n",
    "    perm = np.random.permutation(N)\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "\n",
    "    # 0〜Nまでのデータをバッチサイズごとに使って学習\n",
    "    for i in xrange(0, N, batchsize):\n",
    "        x_batch = x_train[perm[i:i+batchsize]]\n",
    "        y_batch = y_train[perm[i:i+batchsize]]\n",
    "        loss, acc = train(x_batch, y_batch)\n",
    "\n",
    "#         sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n",
    "#         sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n",
    "        sum_loss     += float(loss.data) * batchsize\n",
    "        sum_accuracy += float(acc.data) * batchsize\n",
    "        sys.exit()\n",
    "    # 訓練データの誤差と、正解精度を表示\n",
    "    #print 'train mean loss={}, accuracy={}'.format(sum_loss / N, sum_accuracy / N)\n",
    "\n",
    "    train_loss.append(sum_loss / N)\n",
    "    train_acc.append(sum_accuracy / N)\n",
    "\n",
    "    # evaluation\n",
    "    # テストデータで誤差と、正解精度を算出し汎化性能を確認\n",
    "    sum_accuracy = 0\n",
    "    sum_loss     = 0\n",
    "    for i in xrange(0, N_test, batchsize):\n",
    "        x_batch = x_test[i:i+batchsize]\n",
    "        y_batch = y_test[i:i+batchsize]\n",
    "\n",
    "        # 型を変換\n",
    "        x = Variable(x_batch) \n",
    "        t = Variable(y_batch)\n",
    "        loss, acc = train(x_batch, y_batch, train=False)\n",
    "\n",
    "#         sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n",
    "#         sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n",
    "        sum_loss     += float(loss.data) * batchsize\n",
    "        sum_accuracy += float(acc.data) * batchsize\n",
    "\n",
    "    # テストデータでの誤差と、正解精度を表示\n",
    "    #print 'test  mean loss={}, accuracy={}'.format(sum_loss / N_test, sum_accuracy / N_test)\n",
    "    test_loss.append(sum_loss / N_test)\n",
    "    test_acc.append(sum_accuracy / N_test)\n",
    "\n",
    "    # 学習したパラメーターを保存\n",
    "    l1_W.append(model.l1.W)\n",
    "    l2_W.append(model.l2.W)\n",
    "    l3_W.append(model.l3.W)\n",
    "\n",
    "models.set_model_pkl('{0}.pkl'.format(target))\n",
    "models.set_model_name('{0}.model'.format(target))\n",
    "models.set_optimizer_name('{0}.state'.format(target))\n",
    "models.dump_model(model)\n",
    "models.dump_model_and_optimizer(model, optimizer)\n",
    "    \n",
    "# 精度と誤差をグラフ描画\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(len(train_acc)), train_acc)\n",
    "plt.plot(range(len(test_acc)), test_acc)\n",
    "plt.legend([\"train_acc\",\"test_acc\"],loc=4)\n",
    "plt.title(\"Accuracy of digit recognition.\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
