{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CompareFeatures():\n",
    "    def __init__(self):\n",
    "        #特徴点抽出\n",
    "        # AgastFeature検出器の生成\n",
    "#         self.detector = cv2.AgastFeatureDetector_create()\n",
    "        # FAST検出器の生成\n",
    "#         self.detector = cv2.FastFeatureDetector_create()\n",
    "        # MSER検出器の生成\n",
    "#         self.detector = cv2.MSER_create()\n",
    "\n",
    "        # 特徴量抽出\n",
    "        # A-KAZE検出器の生成\n",
    "#         self.detector = cv2.AKAZE_create()\n",
    "        # BRISK検出器の生成\n",
    "#         self.detector = cv2.BRISK_create()\n",
    "        # KAZE検出器の生成\n",
    "        self.detector = cv2.KAZE_create()\n",
    "        # ORB (Oriented FAST and Rotated BRIEF)検出器の生成\n",
    "#         self.detector = cv2.ORB_create()\n",
    "        # SimpleBlob検出器の生成\n",
    "#         self.detector = cv2.SimpleBlobDetector_create()\n",
    "\n",
    "        # Brute-Force Matcher生成\n",
    "        self.bf=cv2.BFMatcher(cv2.NORM_L1,crossCheck=False)\n",
    "\n",
    "    def load_imgs(self,base_img,match_img, scale=(1,1)):\n",
    "        try:\n",
    "            # 画像\n",
    "            if base_img:\n",
    "                self.base=cv2.imread(base_img)\n",
    "            ma=cv2.imread(match_img)\n",
    "            self.match=cv2.resize(ma, (int(ma.shape[:2][1]/scale[1]),int(ma.shape[:2][0]/scale[1])))\n",
    "        except Exception as e:\n",
    "            print(\"{}\".format(e))\n",
    "\n",
    "    def run_filter(self, img, prev=False):\n",
    "#         after=self.to_grayscale(img)\n",
    "#         after=self.binary_threshold(img)\n",
    "#         after=self.blur(img)\n",
    "#         after=self.mask_blue(img)\n",
    "        after=self.emphasize_edge(img)\n",
    "#         after=self.blur(after)\n",
    "#         after=self.morph(after)\n",
    "        if prev:\n",
    "            self.view_matplot(after)\n",
    "        return after\n",
    "#         return img\n",
    "\n",
    "    def f_match(self, k_num=2, drop_ratio=0.9):\n",
    "        # 特徴を際立たせるため filter処理による前処理を実施\n",
    "        match = self.run_filter(self.match, prev=False)\n",
    "        base = self.run_filter(self.base, prev=False)\n",
    "\n",
    "        # 特徴量の検出と特徴量ベクトルの計算\n",
    "        kp1, des1 = self.detector.detectAndCompute(match, None)\n",
    "        kp2, des2 = self.detector.detectAndCompute(base, None)\n",
    "        print(\"{0} : {1}\".format(len(kp1),len(kp2)))\n",
    "        view_dst = True\n",
    "        if view_dst:\n",
    "            img_kp1 = cv2.drawKeypoints(self.match, kp1, None)\n",
    "            self.view_matplot(img_kp1)\n",
    "            img_kp2 = cv2.drawKeypoints(self.base, kp2, None)\n",
    "            self.view_matplot(img_kp2)        \n",
    "#         kp1 = self.detector.detect(self.match)\n",
    "#         kp2 = self.detector.detect(self.base)\n",
    "\n",
    "        # 特徴量ベクトル同士をBrute-Force＆KNNでマッチング\n",
    "        matches = self.bf.knnMatch(des1, des2, k=k_num)\n",
    "        print(\"Matches origin key points count. : {}\".format(len(matches)))\n",
    "        # データを間引きする\n",
    "        matched = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < drop_ratio * n.distance:\n",
    "#                 print(\"{0} : {1}\".format(m.distance,n.distance))\n",
    "                matched.append([m])\n",
    "        return matched, kp1, kp2\n",
    "\n",
    "    def show_matching(self):\n",
    "        if self.base is None or self.match is None:\n",
    "            print(\"Image has not been loaded yet. Please load imgs\")\n",
    "            raise\n",
    "\n",
    "        matched,kp1,kp2 = self.f_match()\n",
    "        # 対応する特徴点同士を描画\n",
    "        img = cv2.drawMatchesKnn(self.match, kp1, self.base, kp2, matched, None, flags=2)\n",
    "        # opencv\n",
    "#         self.view(img)\n",
    "        # matplot\n",
    "        self.view_matplot(img)\n",
    "\n",
    "    def show_rect(self, IS_PARENT=False):\n",
    "        if self.base is None or self.match is None:\n",
    "            print(\"Image has not been loaded yet. Please load imgs\")\n",
    "            raise\n",
    "        matched,kp1,kp2 = self.f_match()\n",
    "\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks = 50)\n",
    "\n",
    "        MIN_MATCH_COUNT = 4\n",
    "        if len(matched)>=MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([ kp1[m[0].queryIdx].pt for m in matched ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp2[m[0].trainIdx].pt for m in matched ]).reshape(-1,1,2)\n",
    "\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "\n",
    "            h,w,a = self.match.shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,M)\n",
    "            print(dst)\n",
    "            center=(dst[:,0][:,0].mean(),dst[:,0][:,1].mean())\n",
    "            tgt_img=self.base\n",
    "\n",
    "            if 'parent_postion' in dir(self) and len(self.parent_postion):\n",
    "                print(self.parent_postion)\n",
    "                center=(self.parent_postion[0]+dst[:,0][:,0].mean(),self.parent_postion[1]+dst[:,0][:,1].mean())\n",
    "                tgt_img=self.parent_img['base']\n",
    "\n",
    "            self.parent_postion = []\n",
    "            self.parent_img = {}\n",
    "            if IS_PARENT:\n",
    "                self.parent_postion=dst[:,0][0]\n",
    "#                 im[y:y+h, x:x+w]\n",
    "                trim_img = self.base.copy()\n",
    "                print(dst[:,0][0][1])\n",
    "                print(dst[:,0][2][1])\n",
    "                print(dst[:,0][0][0])\n",
    "                print(dst[:,0][2][1])\n",
    "                self.base = trim_img[int(dst[:,0][0][1]):int(dst[:,0][2][1]), int(dst[:,0][0][0]):int(dst[:,0][2][1])]\n",
    "                self.parent_img['base']=self.base.copy()\n",
    "                self.parent_img['match']=self.match.copy()\n",
    "\n",
    "            img2 = cv2.polylines(tgt_img,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "            cv2.circle(img2, center, 5, (255, 0, 0), -1)\n",
    "        else:\n",
    "            print(\"Not enough matches are found - %d/%d\" % (len(matched),MIN_MATCH_COUNT))\n",
    "            matchesMask = None\n",
    "\n",
    "        draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                           singlePointColor = None,\n",
    "                           matchesMask = matchesMask, # draw only inliers\n",
    "                           flags = 2)\n",
    "\n",
    "#         img = cv2.drawMatches(self.match, kp1, self.base, kp2, matched, None, **draw_params)\n",
    "#         img = cv2.drawMatches(self.match, kp1, img2, kp2, matched, None, **draw_params)\n",
    "\n",
    "        # opencv\n",
    "#         self.view(img)\n",
    "        # matplot\n",
    "        self.view_matplot(img2)\n",
    "\n",
    "    def get_postion(self, IS_PARENT=False):\n",
    "        if self.base is None or self.match is None:\n",
    "            print(\"Image has not been loaded yet. Please load imgs\")\n",
    "            raise\n",
    "        matched,kp1,kp2 = self.f_match()\n",
    "\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks = 50)\n",
    "\n",
    "        MIN_MATCH_COUNT = 4\n",
    "        if len(matched)>MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([ kp1[m[0].queryIdx].pt for m in matched ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp2[m[0].trainIdx].pt for m in matched ]).reshape(-1,1,2)\n",
    "\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "\n",
    "            h,w,a = self.match.shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,M)\n",
    "            center=(dst[:,0][:,0].mean(),dst[:,0][:,1].mean())\n",
    "            if 'parent_postion' in dir(self) and len(self.parent_postion):\n",
    "                print(self.parent_postion)\n",
    "                center=(self.parent_postion[0]+dst[:,0][:,0].mean(),self.parent_postion[1]+dst[:,0][:,1].mean())\n",
    "\n",
    "            self.parent_postion = []\n",
    "            if IS_PARENT:\n",
    "                self.parent_postion=dst[:,0][0]\n",
    "            return center\n",
    "\n",
    "        else:\n",
    "            print(\"Not enough matches are found - %d/%d\" % (len(matched),MIN_MATCH_COUNT))\n",
    "            matchesMask = None\n",
    "\n",
    "    def to_grayscale(self,img):\n",
    "        grayed = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return grayed\n",
    "\n",
    "    def binary_threshold(self,img):\n",
    "        grayed = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        under_thresh = 105\n",
    "        upper_thresh = 145\n",
    "        maxValue = 255\n",
    "        th, drop_back = cv2.threshold(grayed, under_thresh, maxValue, cv2.THRESH_BINARY)\n",
    "        th, clarify_born = cv2.threshold(grayed, upper_thresh, maxValue, cv2.THRESH_BINARY_INV)\n",
    "        merged = np.minimum(drop_back, clarify_born)\n",
    "        return merged\n",
    "\n",
    "    def blur(self, img):\n",
    "        filtered = cv2.GaussianBlur(img, (11, 11), 0)\n",
    "        return filtered\n",
    "\n",
    "    def morph(self, img):\n",
    "        kernel = np.ones((3, 3),np.uint8)\n",
    "        opened = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "        return opened\n",
    "\n",
    "    def mask_blue(self, img):\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        blue_min = np.array([100, 170, 200], np.uint8)\n",
    "        blue_max = np.array([120, 180, 255], np.uint8)\n",
    "\n",
    "        blue_region = cv2.inRange(hsv, blue_min, blue_max)\n",
    "        white = np.full(img.shape, 255, dtype=img.dtype)\n",
    "        background = cv2.bitwise_and(white, white, mask=blue_region)  # detected blue area becomes white\n",
    "\n",
    "        inv_mask = cv2.bitwise_not(blue_region)  # make mask for not-blue area\n",
    "        extracted = cv2.bitwise_and(img, img, mask=inv_mask)\n",
    "\n",
    "        masked = cv2.add(extracted, background)\n",
    "\n",
    "        return masked\n",
    "\n",
    "    def emphasize_edge(self,img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        canny_img = cv2.Canny(gray, 50, 110)\n",
    "        return canny_img\n",
    "    \n",
    "    def view(self, img):\n",
    "        # 画像表示\n",
    "        cv2.imshow('img', img)\n",
    "        # キー押下で終了\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    def view_matplot(self, img):\n",
    "        plt.figure(figsize=(16,12))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#         plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stub用テストシナリオ\n",
    "def stub_scenario():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sub_map():\n",
    "    return {\n",
    "        0:{\n",
    "            'base':'base.jpg',\n",
    "            'match':'match.jpg',\n",
    "            'scale':1\n",
    "        },\n",
    "        1:{\n",
    "            'base':'test2.jpg',\n",
    "            'match':'test1_cut1.jpg',\n",
    "            'scale':0.5,\n",
    "            'is_parent':True\n",
    "        },\n",
    "        2:{\n",
    "#             'base':'test1_cut1.jpg',\n",
    "            'base':None,\n",
    "            'scale':1,\n",
    "            'match':'test1_check_box.jpg'\n",
    "        },\n",
    "#         3:{\n",
    "#             'base':'test2.jpg',\n",
    "#             'match':'test1_cut1.jpg'\n",
    "#         }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# テストベースシナリオ\n",
    "SUB_MAP=generate_sub_map()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cf = CompareFeatures()\n",
    "    for key in SUB_MAP:\n",
    "        bs=SUB_MAP[key]\n",
    "        cf.load_imgs(bs['base'],bs['match'],scale=(1,bs['scale']))\n",
    "#         cf.show_rect()\n",
    "#         cf.show_matching()\n",
    "        if 'is_parent' in bs:\n",
    "#             bs['val']=cf.get_postion(IS_PARENT=True)\n",
    "            cf.show_rect(IS_PARENT=True)\n",
    "#             cf.show_matching()\n",
    "            continue\n",
    "#         bs['val']=cf.get_postion()\n",
    "#         cf.show_rect()\n",
    "        cf.show_matching()\n",
    "\n",
    "#     print(SUB_MAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
