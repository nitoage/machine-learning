{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "\n",
    "IMG_SIZE = (180, 320)\n",
    "drop_ratio=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VideoAnalyze():\n",
    "    def __init__(self,video_name):\n",
    "        # 特徴量抽出\n",
    "        # A-KAZE検出器の生成\n",
    "#         self.detector = cv2.AKAZE_create()\n",
    "#         self.detector = cv2.KAZE_create()\n",
    "#         self.detector = cv2.ORB_create()\n",
    "        # Brute-Force Matcher生成\n",
    "#         self.bf=cv2.BFMatcher(cv2.NORM_L1,crossCheck=False)\n",
    "        self.cascade = cv2.CascadeClassifier('cascade.xml') #分類器の指定\n",
    "        self.video_name=video_name\n",
    "        # 8近傍の定義\n",
    "        self.neiborhood8 = np.array([[1, 1, 1],[1, 1, 1],[1, 1, 1]],np.uint8)\n",
    "\n",
    "    def show(self, match):\n",
    "        img1=match['img1']\n",
    "        img2=match['img2']\n",
    "        plt.figure(figsize=(16,12))\n",
    "        # 左\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "        # 右\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def show_matching(self, match):\n",
    "        matched=match['matched']\n",
    "        kp1=match['kp1']\n",
    "        kp2=match['kp2']\n",
    "        img1=match['img1']\n",
    "        img2=match['img2']\n",
    "        # 対応する特徴点同士を描画\n",
    "        img = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matched, None, flags=2)\n",
    "#     img = cv2.drawMatches(img1, kp1, img2, kp2, matched, None, flags=2)\n",
    "        plt.figure(figsize=(16,12))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "\n",
    "#     def compare_frame(self, frame, frame_path):\n",
    "    def compare_frame(self, frame):\n",
    "        # 特徴量の検出と特徴量ベクトルの計算\n",
    "        self.top_match={}\n",
    "        gray_tmp=frame.copy()\n",
    "#         gray_tmp = cv2.cvtColor(tmp, cv2.COLOR_BGR2GRAY)\n",
    "        gray_tmp = cv2.resize(gray_tmp, IMG_SIZE)\n",
    "        gray_tmp=gray_tmp[10:320, 0:180]\n",
    "#         __, target_des = self.detector.detectAndCompute(gray_tmp, None)\n",
    "        y=310\n",
    "        x=180\n",
    "#         target_hist = cv2.calcHist([gray_tmp], [0], None, [256], [0, 256])\n",
    "        target_hist1 = cv2.calcHist([gray_tmp[0:int(y/2), 0:int(x/2)]], [0], None, [256], [0, 256])\n",
    "        target_hist2 = cv2.calcHist([gray_tmp[0:int(y/2), int(x/2):x]], [0], None, [256], [0, 256])\n",
    "        target_hist3 = cv2.calcHist([gray_tmp[int(y/2):y, 0:int(x/2)]], [0], None, [256], [0, 256])\n",
    "        target_hist4 = cv2.calcHist([gray_tmp[int(y/2):y, int(x/2):x]], [0], None, [256], [0, 256])\n",
    "        for path in glob.glob('./base_img/*.png'):\n",
    "            try:\n",
    "    #             matches = bf.match(target_des, d_dict[path]['des'])\n",
    "    #             dist = [m.distance for m in matches]\n",
    "    #             matched = [m for m in matches]\n",
    "    #             matched = sorted(matches, key=lambda x:x.distance)\n",
    "\n",
    "#                 matches = self.bf.knnMatch(target_des, self.d_dict[path]['des'], k=2)\n",
    "#                 dist = []\n",
    "#                 matched=[]\n",
    "#                 for m, n in matches:\n",
    "#                     if m.distance < drop_ratio * n.distance:\n",
    "#                         matched.append([m])\n",
    "#                         dist.append(m.distance)\n",
    "\n",
    "#                 ret = sum(dist) / len(dist)\n",
    "                ret=[]\n",
    "#                 ret.append(cv2.compareHist(target_hist, self.d_dict[path]['hist'], cv2.HISTCMP_CHISQR_ALT))\n",
    "                ret.append(cv2.compareHist(target_hist1, self.d_dict[path]['hist1'], cv2.HISTCMP_CHISQR_ALT))\n",
    "                ret.append(cv2.compareHist(target_hist2, self.d_dict[path]['hist2'], cv2.HISTCMP_CHISQR_ALT))\n",
    "                ret.append(cv2.compareHist(target_hist3, self.d_dict[path]['hist3'], cv2.HISTCMP_CHISQR_ALT))\n",
    "                ret.append(cv2.compareHist(target_hist4, self.d_dict[path]['hist4'], cv2.HISTCMP_CHISQR_ALT))\n",
    "#                 print(\"{0} : {1} : {2}\".format(path, frame_path,sum(ret)))\n",
    "                if not self.top_match or self.top_match['ret'] > sum(ret):\n",
    "                    self.top_match['ret']=sum(ret)\n",
    "                    self.top_match['path']=path\n",
    "                    self.top_match['ts']={'img1':self.d_dict[path]['img'],'img2':gray_tmp}\n",
    "#                     self.top_match['ts']={'matched':matched, 'img1':self.d_dict[path]['img'], \\\n",
    "#                                           'kp1':self.d_dict[path]['kp'],'img2':gray_tmp, 'kp2':__,} \n",
    "            except cv2.error:\n",
    "                ret = 100000\n",
    "\n",
    "#         print(self.top_match['path'], self.top_match['ret'])\n",
    "#         self.show_matching(self.top_match['ts'])\n",
    "#         self.show(self.top_match['ts'])\n",
    "\n",
    "    def create_base_sienario(self, frame, rect, senario_dict, i):\n",
    "        tmp=frame.copy()\n",
    "        for (x, y, w, h) in rect:\n",
    "            if y < 30 or h > 500 or w > 500:\n",
    "                continue\n",
    "            # タップライン\n",
    "            height, width = tmp.shape[:2]\n",
    "            if h < 50:\n",
    "                h_buff=int((50-h)/2)                \n",
    "                y = y - h_buff\n",
    "                h = h + h_buff*2\n",
    "            tap_line=tmp[y:y+h, 0:width]\n",
    "            tap_line_name='./s_img/tap_line_{0}.jpg'.format(i)\n",
    "            cv2.imwrite(tap_line_name, tap_line)\n",
    "            senario_dict[i]={}\n",
    "            senario_dict[i]['line']={}\n",
    "            senario_dict[i]['line']['base']=self.top_match['path']\n",
    "            senario_dict[i]['line']['match']=tap_line_name\n",
    "            senario_dict[i]['line']['is_parent']=True\n",
    "            senario_dict[i]['line']['scale']=1\n",
    "\n",
    "            # タップ位置\n",
    "            if w < 50:\n",
    "                w_buff=int((50-w)/2)\n",
    "                x-=w_buff\n",
    "                w+=w_buff*2\n",
    "            tap_img=tmp[y:y+h, x:x+w]\n",
    "            tap_name='./s_img/tap_{0}.jpg'.format(i)\n",
    "            cv2.imwrite(tap_name, tap_img)\n",
    "            senario_dict[i]['tap']={}\n",
    "            senario_dict[i]['tap']['base']=None\n",
    "            senario_dict[i]['tap']['match']=tap_name\n",
    "            senario_dict[i]['tap']['scale']=1\n",
    "            i+=1\n",
    "        return i\n",
    "\n",
    "    def flame_diff(self,im1,im2,im3,th, tl,blur):\n",
    "        im1=cv2.cvtColor(im1, cv2.COLOR_RGB2GRAY)\n",
    "        im2=cv2.cvtColor(im2, cv2.COLOR_RGB2GRAY)\n",
    "        im3=cv2.cvtColor(im3, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        d1 = cv2.absdiff(im3, im2)\n",
    "        d2 = cv2.absdiff(im2, im1)\n",
    "        diff = cv2.bitwise_and(d1, d2)\n",
    "        \n",
    "        # 差分が閾値より小さければTrue\n",
    "        #mask = np.where((diff<th)&(diff>tl))\n",
    "        mask = diff<th\n",
    "\n",
    "        # 背景画像と同じサイズの配列生成\n",
    "        im_mask = np.empty((im1.shape[0],im1.shape[1]),np.uint8)\n",
    "        im_mask[:][:]=255\n",
    "        # Trueの部分（背景）は黒塗り\n",
    "        im_mask[mask]=0\n",
    "\n",
    "        # 8近傍で膨張処理\n",
    "        im_mask = cv2.dilate(im_mask,\n",
    "                              self.neiborhood8,\n",
    "                              iterations=5)\n",
    "        # ノイズ除去\n",
    "        im_mask = cv2.medianBlur(im_mask,blur)\n",
    "\n",
    "        return  im_mask\n",
    "\n",
    "    def find_rect(self,image):\n",
    "    #  hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV_FULL)\n",
    "    #  h = hsv[:, :, 0]\n",
    "    #  s = hsv[:, :, 1]\n",
    "    #  mask = np.zeros(h.shape, dtype=np.uint8)\n",
    "    #  mask[((h < 20) | (h > 200)) & (s > 128)] = 255\n",
    "        _, contours, _ = cv2.findContours(image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        rects = []\n",
    "        for contour in contours:\n",
    "            approx = cv2.convexHull(contour)\n",
    "            rect = cv2.boundingRect(approx)\n",
    "            rects.append(np.array(rect))\n",
    "        return rects\n",
    "    \n",
    "    def main(self):\n",
    "        senario_dict={}\n",
    "        self.d_dict={}\n",
    "        for img_path in glob.glob('./base_img/*.png'):\n",
    "            gray_img = cv2.imread(img_path)\n",
    "#             gray_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            gray_img = cv2.resize(gray_img, IMG_SIZE)\n",
    "            gray_img=gray_img[10:320, 0:180]\n",
    "#             kp, des = self.detector.detectAndCompute(gray_img, None)          \n",
    "            self.d_dict[img_path]={}\n",
    "#             self.d_dict[img_path]['des']=des\n",
    "#             self.d_dict[img_path]['kp']=kp\n",
    "            self.d_dict[img_path]['img']=gray_img\n",
    "            y=310\n",
    "            x=180\n",
    "#             self.d_dict[img_path]['hist'] = cv2.calcHist([gray_img], [0], None, [256], [0, 256])\n",
    "            self.d_dict[img_path]['hist1'] = cv2.calcHist([gray_img[0:int(y/2), 0:int(x/2)]], [0], None, [256], [0, 256])\n",
    "            self.d_dict[img_path]['hist2'] = cv2.calcHist([gray_img[0:int(y/2), int(x/2):x]], [0], None, [256], [0, 256])\n",
    "            self.d_dict[img_path]['hist3'] = cv2.calcHist([gray_img[int(y/2):y, 0:int(x/2)]], [0], None, [256], [0, 256])\n",
    "            self.d_dict[img_path]['hist4'] = cv2.calcHist([gray_img[int(y/2):y, int(x/2):x]], [0], None, [256], [0, 256])\n",
    "        i=0\n",
    "        ct=0\n",
    "        is_print=True\n",
    "        cap = cv2.VideoCapture(self.video_name)\n",
    "        _, frame1 = cap.read()\n",
    "        _, frame2 = cap.read()\n",
    "        ret, frame3 = cap.read()\n",
    "        #print(\"{0}: is Open {1}\".format(self.video_name, cap.isOpened()))\n",
    "#         prev_detect=np.array([[-1,-1,-1,-1]])\n",
    "        while ret:\n",
    "#         for frame_path in glob.glob('./cat_img/*.png'):\n",
    "#             ret, frame = cap.read()\n",
    "#             frame = cv2.imread(frame_path)\n",
    "#             if frame is None:\n",
    "#                 break\n",
    "            frame_fs = self.flame_diff(frame1.copy(),frame2.copy(),frame3.copy(), 5, 0,5)\n",
    "            rect = self.find_rect(frame_fs.copy())\n",
    "            ct+=1\n",
    "            if ct>10:\n",
    "                is_print=True\n",
    "\n",
    "            if is_print and len(rect) and len(rect) <3:\n",
    "                is_print=False\n",
    "                ct=0\n",
    "                #print(len(rect), rect)\n",
    "                label = cv2.connectedComponentsWithStats(frame_fs)\n",
    "                # ラベルの個数nと各ラベルの重心座標cogを取得\n",
    "                n = label[0] - 1\n",
    "                cog = np.delete(label[3], 0, 0)\n",
    "                #print(cog)\n",
    "                #print(int(cog[:,0].mean()))\n",
    "                #print(int(cog[:,1].mean()))\n",
    "                #break\n",
    "                # 重心に赤円を描く\n",
    "                #for i in range(n):\n",
    "                #im2 = cv2.circle(im2,(int(rect[:,0].mean()),int(rect[:,1].mean())), 40, (0,0,0), -1)\n",
    "                test2 = cv2.circle(frame2.copy(),(int(cog[:,0].mean()),int(cog[:,1].mean())), 40, (0,0,0), -1)\n",
    "                cv2.imwrite(\"./test/test{0:0>3}.jpg\".format(i) ,test2)\n",
    "\n",
    "            #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # detectMultiScale(Mat image, MatOfRect objects, double scaleFactor, int minNeighbors, int flag, Size minSize, Size maxSize)\n",
    "#             detect = self.cascade.detectMultiScale(frame, 1.1, 3) #物体の検出\n",
    "#             print (detect,prev_detect)\n",
    "#             if type(detect) is tuple or (detect == prev_detect).all():\n",
    "#                 prev_detect= np.array([[-1,-1,-1,-1]]) if type(detect) is tuple else detect\n",
    "#                 continue\n",
    "#             print(detect)\n",
    "#             self.compare_frame(frame, frame_path)\n",
    "                self.compare_frame(frame2)\n",
    "                i=self.create_base_sienario(frame2, rect, senario_dict, i)\n",
    "\n",
    "            frame1 = frame2\n",
    "            frame2 = frame3\n",
    "            ret, frame3 = cap.read()\n",
    "#             prev_detect=detect\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        # print(senario_dict)\n",
    "#         cap.release()\n",
    "        return senario_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_0.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_0.jpg', 'scale': 1}}, 1: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_1.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_1.jpg', 'scale': 1}}, 2: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_2.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_2.jpg', 'scale': 1}}, 3: {'line': {'base': './base_img/Screenshot_20170620-194348.png', 'match': './s_img/tap_line_3.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_3.jpg', 'scale': 1}}, 4: {'line': {'base': './base_img/Screenshot_20170620-194348.png', 'match': './s_img/tap_line_4.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_4.jpg', 'scale': 1}}, 5: {'line': {'base': './base_img/Screenshot_20170620-194348.png', 'match': './s_img/tap_line_5.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_5.jpg', 'scale': 1}}, 6: {'line': {'base': './base_img/Screenshot_20170620-194348.png', 'match': './s_img/tap_line_6.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_6.jpg', 'scale': 1}}, 7: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_7.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_7.jpg', 'scale': 1}}, 8: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_8.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_8.jpg', 'scale': 1}}, 9: {'line': {'base': './base_img/Screenshot_20170620-194353.png', 'match': './s_img/tap_line_9.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_9.jpg', 'scale': 1}}, 10: {'line': {'base': './base_img/Screenshot_20170620-194353.png', 'match': './s_img/tap_line_10.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_10.jpg', 'scale': 1}}, 11: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_11.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_11.jpg', 'scale': 1}}, 12: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_12.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_12.jpg', 'scale': 1}}, 13: {'line': {'base': './base_img/Screenshot_20170620-194415.png', 'match': './s_img/tap_line_13.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_13.jpg', 'scale': 1}}, 14: {'line': {'base': './base_img/Screenshot_20170620-194415.png', 'match': './s_img/tap_line_14.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_14.jpg', 'scale': 1}}, 15: {'line': {'base': './base_img/Screenshot_20170620-194357.png', 'match': './s_img/tap_line_15.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_15.jpg', 'scale': 1}}, 16: {'line': {'base': './base_img/Screenshot_20170620-194357.png', 'match': './s_img/tap_line_16.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_16.jpg', 'scale': 1}}, 17: {'line': {'base': './base_img/Screenshot_20170620-194357.png', 'match': './s_img/tap_line_17.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_17.jpg', 'scale': 1}}, 18: {'line': {'base': './base_img/Screenshot_20170620-194357.png', 'match': './s_img/tap_line_18.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_18.jpg', 'scale': 1}}, 19: {'line': {'base': './base_img/Screenshot_20170620-194401.png', 'match': './s_img/tap_line_19.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_19.jpg', 'scale': 1}}, 20: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_20.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_20.jpg', 'scale': 1}}, 21: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_21.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_21.jpg', 'scale': 1}}, 22: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_22.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_22.jpg', 'scale': 1}}, 23: {'line': {'base': './base_img/Screenshot_20170620-194222.png', 'match': './s_img/tap_line_23.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_23.jpg', 'scale': 1}}, 24: {'line': {'base': './base_img/Screenshot_20170620-194222.png', 'match': './s_img/tap_line_24.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_24.jpg', 'scale': 1}}, 25: {'line': {'base': './base_img/Screenshot_20170620-194222.png', 'match': './s_img/tap_line_25.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_25.jpg', 'scale': 1}}, 26: {'line': {'base': './base_img/Screenshot_20170620-194222.png', 'match': './s_img/tap_line_26.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_26.jpg', 'scale': 1}}, 27: {'line': {'base': './base_img/Screenshot_20170620-194237.png', 'match': './s_img/tap_line_27.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_27.jpg', 'scale': 1}}, 28: {'line': {'base': './base_img/Screenshot_20170620-194237.png', 'match': './s_img/tap_line_28.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_28.jpg', 'scale': 1}}, 29: {'line': {'base': './base_img/Screenshot_20170620-194237.png', 'match': './s_img/tap_line_29.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_29.jpg', 'scale': 1}}, 30: {'line': {'base': './base_img/Screenshot_20170620-194222.png', 'match': './s_img/tap_line_30.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_30.jpg', 'scale': 1}}, 31: {'line': {'base': './base_img/Screenshot_20170620-194415.png', 'match': './s_img/tap_line_31.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_31.jpg', 'scale': 1}}, 32: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_32.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_32.jpg', 'scale': 1}}, 33: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_33.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_33.jpg', 'scale': 1}}, 34: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_34.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_34.jpg', 'scale': 1}}, 35: {'line': {'base': './base_img/Screenshot_20170620-194415.png', 'match': './s_img/tap_line_35.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_35.jpg', 'scale': 1}}, 36: {'line': {'base': './base_img/Screenshot_20170620-194415.png', 'match': './s_img/tap_line_36.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_36.jpg', 'scale': 1}}, 37: {'line': {'base': './base_img/Screenshot_20170620-194215.png', 'match': './s_img/tap_line_37.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_37.jpg', 'scale': 1}}, 38: {'line': {'base': './base_img/Screenshot_20170620-194222.png', 'match': './s_img/tap_line_38.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_38.jpg', 'scale': 1}}, 39: {'line': {'base': './base_img/Screenshot_20170620-194415.png', 'match': './s_img/tap_line_39.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_39.jpg', 'scale': 1}}, 40: {'line': {'base': './base_img/Screenshot_20170620-194415.png', 'match': './s_img/tap_line_40.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_40.jpg', 'scale': 1}}, 41: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_41.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_41.jpg', 'scale': 1}}, 42: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_42.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_42.jpg', 'scale': 1}}, 43: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_43.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_43.jpg', 'scale': 1}}, 44: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_44.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_44.jpg', 'scale': 1}}, 45: {'line': {'base': './base_img/Screenshot_20170620-194231.png', 'match': './s_img/tap_line_45.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_45.jpg', 'scale': 1}}, 46: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_46.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_46.jpg', 'scale': 1}}, 47: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_47.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_47.jpg', 'scale': 1}}, 48: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_48.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_48.jpg', 'scale': 1}}, 49: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_49.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_49.jpg', 'scale': 1}}, 50: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_50.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_50.jpg', 'scale': 1}}, 51: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_51.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_51.jpg', 'scale': 1}}, 52: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_52.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_52.jpg', 'scale': 1}}, 53: {'line': {'base': './base_img/Screenshot_20170620-194333.png', 'match': './s_img/tap_line_53.jpg', 'is_parent': True, 'scale': 1}, 'tap': {'base': None, 'match': './s_img/tap_53.jpg', 'scale': 1}}}\n"
     ]
    }
   ],
   "source": [
    "# テストベースシナリオ\n",
    "if __name__ == '__main__':\n",
    "    va=VideoAnalyze('./2.mp4')\n",
    "    senario_dict=va.main()\n",
    "    print(senario_dict)\n",
    "    f = open('{0}.json'.format(\"base_senario\"), 'w')\n",
    "    json.dump(senario_dict, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
